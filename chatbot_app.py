import streamlit as st
import os
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_chroma import Chroma 
from langchain.chains import RetrievalQA 
# ----------------- YAPILANDIRMA -----------------
PERSIST_DIR = "./chroma_db_ai" # VektÃ¶r veritabanÄ± klasÃ¶rÃ¼nÃ¼n konumu

# 1. Sayfa AyarlarÄ± (Web ArayÃ¼zÃ¼)
st.set_page_config(page_title="EndÃ¼stri MÃ¼hendisliÄŸi AsistanÄ± (Gemini RAG)")
st.title("ğŸ‘¨â€ğŸ’» EndÃ¼stri MÃ¼hendisliÄŸi Ã–ÄŸrenci AsistanÄ±")
st.markdown("Ders notlarÄ±na dayalÄ± YÃ¶neylem, Ãœretim ve Ergonomi sorularÄ±nÄ± yanÄ±tlar.")

# Hata KontrolÃ¼ (API AnahtarÄ±)
gemini_api_key = os.environ.get("GOOGLE_API_KEY")
if not gemini_api_key:
    st.error("HATA: GOOGLE_API_KEY ortam deÄŸiÅŸkeni ayarlanmamÄ±ÅŸ. CMDâ€™de `set GOOGLE_API_KEY=...` komutuyla ayarlayÄ±n.")
    st.stop()

@st.cache_resource
def load_rag_chain(api_key):
    """RAG zincirinin bileÅŸenlerini yÃ¼kler."""
    if not os.path.isdir(PERSIST_DIR):
        st.error(f"HATA: VektÃ¶r veritabanÄ± klasÃ¶rÃ¼ ({PERSIST_DIR}) bulunamadÄ±. LÃ¼tfen Ã¶nce 'python rag_pipeline.py' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.")
        st.stop()

    # 1. Embedding ve Vector Store
    embeddings = GoogleGenerativeAIEmbeddings(model="text-embedding-004", api_key=api_key)
    vectorstore = Chroma(persist_directory=PERSIST_DIR, embedding_function=embeddings)
    
    # Retriever objesini alÄ±yoruz (search_kwargs={"k": 4} varsayÄ±labilir)
    retriever = vectorstore.as_retriever() 

    # 2. LLM
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.1, api_key=api_key)

    # 3. Sorgu iÅŸleme fonksiyonu (RAG mantÄ±ÄŸÄ±nÄ± burada uyguluyoruz)
    def answer_question(question: str):
        # DÃ¼zeltme: Retriever'Ä± doÄŸrudan Ã§aÄŸrÄ± (.invoke) olarak kullanÄ±yoruz.
        docs = retriever.invoke(question) 
        
        # AlakalÄ± dÃ¶kÃ¼manlarÄ±n iÃ§eriÄŸini birleÅŸtiriyoruz
        context = "\n\n".join([doc.page_content for doc in docs])

        # Nihai Prompt'u oluÅŸtur
        prompt_content = (
            "Sen bir EndÃ¼stri MÃ¼hendisliÄŸi Ã¶ÄŸrencisi asistanÄ±sÄ±n.\n"
            "YalnÄ±zca saÄŸlanan ders notlarÄ±na dayanarak sorularÄ± cevapla.\n"
            "CevabÄ±n mantÄ±klÄ±, akademik ve kÄ±sa olsun.\n"
            "EÄŸer bilgi mevcut deÄŸilse, 'Elimdeki ders notlarÄ±nda bu konu hakkÄ±nda bilgi bulunmamaktadÄ±r.' diye yanÄ±tla.\n\n"
            f"Context:\n{context}\n\n"
            f"Soru: {question}"
        )

        # Gemini LLM'e gÃ¶nder ve cevabÄ± al
        response = llm.invoke(prompt_content)
        return response.content

    # Zincir yerine, doÄŸrudan sorgulama fonksiyonunu dÃ¶ndÃ¼rÃ¼yoruz
    return answer_question 

# Zinciri yÃ¼kle
try:
    qa_function = load_rag_chain(gemini_api_key)
    st.success("âœ… VeritabanÄ± ve Gemini baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±!")
except Exception as e:
    st.error(f"RAG Zinciri YÃ¼klenirken Hata: {e}")
    st.stop()

# --- Chatbot ArayÃ¼zÃ¼ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Ders notlarÄ±nÄ±zla ilgili bir soru sorun..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Ders notlarÄ±nda arama yapÄ±lÄ±yor..."):
            try:
                # Sorgulama fonksiyonunu doÄŸrudan Ã§aÄŸÄ±r
                assistant_response = qa_function(prompt)
            except Exception as e:
                assistant_response = f"âš ï¸ RAG Zinciri Ã‡alÄ±ÅŸma HatasÄ±: {e}"
            st.markdown(assistant_response)

    st.session_state.messages.append({"role": "assistant", "content": assistant_response})